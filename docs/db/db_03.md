# 部分 II. Search Engine

## 第 8 章 Elasticsearch

http://www.elasticsearch.org/

## 安装 Elasticsearch

### 6.x 安装

安装 6.x 仓库

```

curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elastic/elastic-6.x.sh | bash			

```

安装 6.x 包

```

yum install elasticsearch			

```

### 单机模式 (适用于开发环境) 5.x

使用 Netkiller OSCM 一键安装 Elasticsearch 5.6.0

```
# Java
curl -s https://raw.githubusercontent.com/oscm/shell/master/lang/java/openjdk/java-1.8.0-openjdk.sh | bash

# Install
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/elasticsearch-5.x.sh | bash

# Bind 0.0.0.0
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/network.bind_host.sh | bash

# Auto create index
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/action.auto_create_index.sh | bash

# elasticsearch-analysis-ik

curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/5.5/elasticsearch-analysis-ik-5.6.0.sh | bash

```

通常 elasticsearch-analysis-ik 的版本会比 elasticsearch 慢一个版本，所以请使用下面命令查看版本是否一致，如果不一致可以修改 plugin-descriptor.properties 配置文件，使其一致。

```
root@netkiller /usr/share/elasticsearch/plugins/ik % grep ^version plugin-descriptor.properties
version=5.5.1

```

启动后使用 jps 命令检查进城是否工作正常

```
root@netkiller /var/log/elasticsearch % jps | grep Elasticsearch
9706 Elasticsearch

root@netkiller /var/log/elasticsearch % ss -lnt | grep 9200
LISTEN     0      128    127.0.0.1:9200                     *:*

```

### Elasticsearch Cluster 5.x

集群模式需要两个以上的节点，通常是一个 master 节点，多个 data 节点

首先在所有节点上安装 elasticsearch，然后配置各节点的配置文件，对于 5.5.1 不需要配置决定哪些节点属于 master 节点 或者 data 节点。

```
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/elasticsearch-5.x.sh | bash			

```

配置文件

```
cluster.name: elasticsearch-cluster # 配置集群名称,所有服务器服务器保持一致

node.name: node-1 # 每个节点唯一标识，每个节点只需改动这里，一次递增 node-1, node-2, node-3 ...

network.host: 0.0.0.0

discovery.zen.ping.unicast.hosts: ["172.16.0.20", "172.16.0.21","172.16.0.22"]  # 所有节点的 IP 地址写在这里

discovery.zen.minimum_master_nodes: 3 # 可以作为 master 的节点总数，有多少个节点就写多少

http.cors.enabled: true
http.cors.allow-origin: "*"

```

查看节点状态，使用 curl 工具: curl 'http://localhost:9200/_nodes/process?pretty'

```
root@netkiller /var/log/elasticsearch % curl 'http://localhost:9200/_nodes/process?pretty'
{
  "_nodes" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "cluster_name" : "my-application",
  "nodes" : {
    "-lnKCmBXRpiwExLns0jc9g" : {
      "name" : "node-1",
      "transport_address" : "10.104.3.2:9300",
      "host" : "10.104.3.2",
      "ip" : "10.104.3.2",
      "version" : "5.5.1",
      "build_hash" : "19c13d0",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 23669,
        "mlockall" : false
      }
    },
    "WVsgYi2HT8GWnZU1kUwFwA" : {
      "name" : "node-2",
      "transport_address" : "10.186.7.221:9300",
      "host" : "10.186.7.221",
      "ip" : "10.186.7.221",
      "version" : "5.5.1",
      "build_hash" : "19c13d0",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 12641,
        "mlockall" : false
      }
    }
  }
}

```

启动节点后回生成 cluster.name 为文件名的日志文件。

谁先启动谁讲成为 master

```
[2017-08-11T17:42:46,018][INFO ][o.e.c.s.ClusterService   ] [node-1] new_master {node-1}{-lnKCmBXRpiwExLns0jc9g}{rZcJDIynSzq2Td3yP2kN5A}{10.104.3.2}{10.104.3.2:9300}, added {{node-2}{WVsgYi2HT8GWnZU1kUwFwA}{X13ShUpAQa2zA1Mgcsm3bQ}{10.186.7.221}{10.186.7.221:9300},}, reason: zen-disco-elected-as-master ([1] nodes joined)[{node-2}{WVsgYi2HT8GWnZU1kUwFwA}{X13ShUpAQa2zA1Mgcsm3bQ}{10.186.7.221}{10.186.7.221:9300}]			

```

如果 master 出现故障，其他节点会接管

```
[2017-08-11T17:44:52,797][INFO ][o.e.c.s.ClusterService   ] [node-2] master {new {node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300}}, removed {{node-1}{-lnKCmBXRpiwExLns0jc9g}{rZcJDIynSzq2Td3yP2kN5A}{10.104.3.2}{10.104.3.2:9300},}, added {{node-1}{-lnKCmBXRpiwExLns0jc9g}{odnoG9kpQpeX1ltx5KYTSw}{10.104.3.2}{10.104.3.2:9300},}, reason: zen-disco-elected-as-master ([1] nodes joined)[{node-1}{-lnKCmBXRpiwExLns0jc9g}{odnoG9kpQpeX1ltx5KYTSw}{10.104.3.2}{10.104.3.2:9300}]
[2017-08-11T17:44:53,184][INFO ][o.e.c.r.DelayedAllocationService] [node-2] scheduling reroute for delayed shards in [59.5s] (11 delayed shards)
[2017-08-11T17:44:53,929][INFO ][o.e.c.r.a.AllocationService] [node-2] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[information][0]] ...]).		

```

master 节点恢复上线会提示

```
[2017-08-11T17:44:52,855][INFO ][o.e.c.s.ClusterService   ] [node-1] detected_master {node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300}, added {{node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300},}, reason: zen-disco-receive(from master [master {node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300} committed version [44]])

```

### 负载均衡配置

首先安装 nginx, 这里使用 Netkiller OSCM 一键安装脚本完成。

```
# curl -s https://raw.githubusercontent.com/oscm/shell/master/web/nginx/stable/nginx.sh | bash

```

因为 elasticsearch 没有用户认证机制我们通常在内网访问他。如果对外提供服务需要增加用户认证。

```

# printf "neo:$(openssl passwd -crypt s3cr3t)n" > /etc/nginx/passwords 			

```

创建 nginx 配置文件 /etc/nginx/conf.d/elasticsearch.conf

```
upstream elasticsearch {
	server 172.16.0.10:9200;
	server 172.16.0.20:9200;
	server 172.16.0.30:9200;

	keepalive 15;
}

server {
	listen 9200;
	server_name so.netkiller.cn;

	charset utf-8;
    access_log /var/log/nginx/so.netkiller.cn.access.log;
    error_log /var/log/nginx/so.netkiller.cn.error.log;

	auth_basic "Protected Elasticsearch";
	auth_basic_user_file passwords;

	location ~* ^(/_cluster|/_nodes) {
		return 403;
		break;
	}
    location ~* _(open|close) {
            return 403;
            break;
    }
	location / {

		if ($request_filename ~ _shutdown) {
		    return 403;
		    break;
		}

        if ($request_method !~ ^(GET|HEAD|POST)$) {
			return 403;
		}

		proxy_pass http://elasticsearch;
		proxy_http_version 1.1;
		proxy_set_header Connection "Keep-Alive";
		proxy_set_header Proxy-Connection "Keep-Alive";
	}

}

```

反复使用下面方法请求，最终你会发现 total_opened 会达到你的 nginx 配置数量

```
$ curl 'http://test:test@localhost:9200/_nodes/stats/http?pretty' | grep total_opened
# "total_opened" : 15			

```

上面的例子适用于绝大多数场景。

例 8.1. Elasticsearch master / slave

```

upstream elasticsearch {
	server 172.16.0.10:9200;
	server 172.16.0.20:9200 backup;

	keepalive 15;
}

server {
	listen 9200;
	server_name so.netkiller.cn;

	auth_basic "Protected Elasticsearch";
	auth_basic_user_file passwords;

	location ~* ^(/_cluster|/_nodes) {
		return 403;
		break;
	} 

	location / {

		if ($request_filename ~ _shutdown) {
		    return 403;
		    break;
		}
		if ($request_method !~ "HEAD") {
          return 403;
          break;
        }
        if ($request_method ~ "DELETE") {
          return 403;
          break;
        }

		proxy_pass http://elasticsearch;
		proxy_http_version 1.1;
		proxy_set_header Connection "Keep-Alive";
		proxy_set_header Proxy-Connection "Keep-Alive";
	}

}

```

通过 limit_except 可以控制访问权限，例如删除操作。

```

limit_except PUT {
	allow 192.168.1.1;
	deny all;
}
limit_except DELETE {
	allow 192.168.1.1;
	deny all;
}

```

### 安装指定版本的 Elasticsearch

使用 yum 安装默认为最新版本，我们常常会遇到一个问题 elasticsearch-analysis-ik 的版本晚于 Elasticsearch。如果使用 yum 安装 Elasticsearch 可能 elasticsearch-analysis-ik 插件不支持这个版本，有些版本的 elasticsearch-analysis-ik 可以修改插件配置文件中的版本号，使其与 elasticsearch 版本相同，可以欺骗 elasticsearch 跳过版本不一致异常。

最佳的解决方案是去 [elasticsearch-analysis-ik github](https://github.com/medcl/elasticsearch-analysis-ik) 找到兼容的版本，安装我们安装 elasticsearch-analysis-ik 的版本需求来指定安装 elasticsearch

```
Versions

IK version	ES version
master	5.x -> master
5.6.0	5.6.0
5.5.3	5.5.3
5.4.3	5.4.3
5.3.3	5.3.3
5.2.2	5.2.2
5.1.2	5.1.2
1.10.1	2.4.1
1.9.5	2.3.5
1.8.1	2.2.1
1.7.0	2.1.1
1.5.0	2.0.0
1.2.6	1.0.0
1.2.5	0.90.x
1.1.3	0.20.x
1.0.0	0.16.2 -> 0.19.0			

```

最新版是 elasticsearch 5.6.1 但分词插件 elasticsearch-analysis-ik 仅能支持到 elasticsearch 版本是 5.6.0

```
root@netkiller /var/log % yum --showduplicates list elasticsearch | expand | tail
Repository epel is listed more than once in the configuration  
elasticsearch.noarch                 5.5.3-1                  elasticsearch-5.x     
elasticsearch.noarch                 5.6.0-1                  elasticsearch-5.x   
elasticsearch.noarch                 5.6.1-1                  elasticsearch-5.x 

```

安装 5.6.0

```
# yum install elasticsearch-5.6.0-1

Loaded plugins: fastestmirror, langpacks
Repository epel is listed more than once in the configuration
Loading mirror speeds from cached hostfile
Resolving Dependencies
--> Running transaction check
---> Package elasticsearch.noarch 0:5.6.0-1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==========================================================================================================================================================================================================
 Package                                            Arch                                        Version                                      Repository                                              Size
==========================================================================================================================================================================================================
Installing:
 elasticsearch                                      noarch                                      5.6.0-1                                      elasticsearch-5.x                                       32 M

Transaction Summary
==========================================================================================================================================================================================================
Install  1 Package

Total download size: 32 M
Installed size: 36 M
Is this ok [y/d/N]: y

```

### Plugin

Elasticsearch 提供了插件管理命令 elasticsearch-plugin

```
root@netkiller ~ % /usr/share/elasticsearch/bin/elasticsearch-plugin -h
A tool for managing installed elasticsearch plugins

Commands
--------
list - Lists installed elasticsearch plugins
install - Install a plugin
remove - removes a plugin from Elasticsearch

Non-option arguments:
command              

Option         Description        
------         -----------        
-h, --help     show help          
-s, --silent   show minimal output
-v, --verbose  show verbose output			

```

#### elasticsearch-analysis-ik

安装插件

```
root@netkiller ~ % /usr/share/elasticsearch/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
-> Downloading https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
[=================================================] 100%   
-> Installed analysis-ik

```

```
curl -XPOST http://localhost:9200/index/fulltext/_mapping -d'
{
        "properties": {
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            }
        }

}'			

```

#### elasticsearch-analysis-pinyin

https://github.com/medcl/elasticsearch-analysis-pinyin

## 文档 API

### 快速上手

文档通过 _index、_type、_id 元数据(metadata)，确定 URL 唯一

```

GET /<_index>/<_type>/<_id>		

```

```
# curl -XPUT 'http://localhost:9200/website/profile/1' -d '{
	"name" : "neo",
	"nickname" : "netkiller",
	"age" : "35",
	"message" : "Helloworld !!!"
}'

# curl -XGET 'http://localhost:9200/website/profile/1?pretty'
{
  "_index" : "website",
  "_type" : "profile",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "name" : "neo",
    "nickname" : "netkiller",
    "age" : "35",
    "message" : "Helloworld !!!"
  }
}

# curl -XPUT 'http://localhost:9200/website/blog/1?pretty' -d '{
>   "title": "My first blog entry",
>   "text":  "Just trying this out...",
>   "date":  "2014/01/01"
> }'
{
  "_index" : "website",
  "_type" : "blog",
  "_id" : "1",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}

```

后面会详细讲解 PUT 与 GET 的使用方法以及相关参数

### 写入 PUT/POST

通过 PUT 写入数据

```
[root@localhost ~]# curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
>     "user" : "kimchy",
>     "post_date" : "2009-11-15T14:12:12",
>     "message" : "trying out Elasticsearch"
> }'
{"_index":"twitter","_type":"tweet","_id":"1","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}			

```

使用 UUID 替代 _id, 注意使用 UUID 必须使用 POST 方式提交，不能使用 PUT。

```
curl -XPOST 'http://localhost:9200/website/news/?pretty' -d '{
  "title": "My first news entry",
  "text":  "Just trying this out..."
}'
{
  "_index" : "website",
  "_type" : "news1",
  "_id" : "AVY0RJrvJRTrBLpmYzBH",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}

curl -XGET 'http://localhost:9200/website/news/AVY0RJrvJRTrBLpmYzBH?pretty'

```

提交后会输出 "_id" : "AVY0RJrvJRTrBLpmYzBH"，查询时将此放到放到 URL 中即可。

### 获取 GET

通过 GET 读取数据

```
[root@localhost ~]# curl -XGET 'http://localhost:9200/twitter/tweet/1'
{"_index":"twitter","_type":"tweet","_id":"1","_version":1,"found":true,"_source":{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}}

```

#### _source

只返回 _source 数据，去掉元数据

```
# curl -XGET 'http://localhost:9200/website/news1/AVY0Q4SqdtH0Up0t-WB2/_source?pretty'
{
  "title" : "My first news entry",
  "text" : "Just trying this out..."
}

```

选择字段 _source=title，超过一个字段使用逗号分隔 _source=title,text。

```

# curl -XGET 'http://localhost:9200/website/news1/AVY0Q4SqdtH0Up0t-WB2?_source=title&pretty'
{
  "_index" : "website",
  "_type" : "news1",
  "_id" : "AVY0Q4SqdtH0Up0t-WB2",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "title" : "My first news entry"
  }
}

# curl -XGET 'http://localhost:9200/website/news1/AVY0Q4SqdtH0Up0t-WB2?_source=title,text&pretty'
{
  "_index" : "website",
  "_type" : "news1",
  "_id" : "AVY0Q4SqdtH0Up0t-WB2",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "text" : "Just trying this out...",
    "title" : "My first news entry"
  }
}

```

### 检查记录是否存在

```
[root@localhost elasticsearch]# curl -i -XHEAD http://localhost:9200/website/blog/1
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

[root@localhost elasticsearch]# curl -i -XHEAD http://localhost:9200/website/blog/100
HTTP/1.1 404 Not Found
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

```

HTTP/1.1 200 OK 表示已经找到你要的数据

HTTP/1.1 404 Not Found 表示数据不存在

### 删除 Delete

删除 _index

```
curl -XDELETE http://localhost:9200/information/?pretty

```

删除 _mapping

```
curl -XDELETE http://localhost:9200/information/news/_mapping?pretty			

```

删除对象

```
curl -XDELETE http://localhost:9200/information/news/1?pretty			

```

### 参数

#### pretty 格式化 json

```
# curl -XGET 'http://localhost:9200/twitter/tweet/1?pretty'
{
  "_index" : "twitter",
  "_type" : "tweet",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
  }
}			

```

## 搜索

搜索所有内容

```
# curl -XGET 'http://localhost:9200/_search?pretty'	
# curl -XGET 'http://localhost:9200/_all/_search?pretty'			

```

指定 _index 搜索

```
# curl -XGET 'http://localhost:9200/website/_search?pretty'
# curl -XGET 'http://localhost:9200/website/news/_search?pretty'

```

指定 _type 搜索

```
# curl -XGET 'http://localhost:9200/website,twitter/_search?pretty'
# curl -XGET 'http://localhost:9200/website/news,blog/_search?pretty'
# curl -XGET 'http://localhost:9200/website,twitter/news,blog/_search?pretty'

```

所有 _index 包含指定 _type 搜索

```
# curl -XGET 'http://localhost:9200/_all/news,blog/_search?pretty'

```

### URL 搜索

字符串搜索

```

# curl -XGET 'http://localhost:9200/_all/_search?q=neo&pretty'

```

同时满足两个条件

```
+name:neo +age:30

```

查找 name 为 mary 或者 john 的数据

```
+name:(mary john)

```

查询姓名是 neo 或者 jam 并且年龄小于 30 岁同时 1980-09-10 之后出生的

```

+name:(neo jam) +age:<30 +date:>1980-09-10

```

### 分页

该功能与 SQL 的 LIMIT 关键字结果一样，Elasticsearch 接受 size 和 from 两个参数参数：

size: 返回结果集数量，默认 10，用法与 SQL 中的 Limit 相同

from: 偏移量，默认 0，用法与 SQL 中的 Offset 相同

如果你想每页显示 10 个结果，那么请求如下：

```

第一页 GET /_search?size=10
第二页 GET /_search?size=10&from=10
第三页 GET /_search?size=10&from=20

```

## Query DSL

### match 匹配

```
curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"query" : {
		"match" : {
			"tag" : "美"   
		}
	}
}
'

```

### multi_match 多字段匹配

multi_match 实现多字段查询

```
curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"query": {
	    "multi_match": {
		    "query":      "国际",
		    "type":       "cross_fields",
		    "fields":     [ "title", "content" ],
		    "operator":   "and"
	    }
	},
	"from": 0,
	"size": 20,
	"_source":["id","title","ctime"],
	"sort": [
	   {
	      "ctime": {"order": "desc"}
	   }
	]

}
'

```

### Query bool 布尔条件

Elasticsearch 提供三个布尔条件

must： AND

must_not：NOT

should：OR

#### must

查询必须满足 tags=天气 and title 包含 台风关键字

```

curl -XPOST http://test:123456@so.netkiller.cn/information/article/_search?pretty  -d'
{
  "query": {
    "bool": {
      "must": [
        { "match": { "tags" : "天气" }},
        { "match": { "title": "台风"   }}
      ]
    }
  },
 "_source":["id","title","ctime"],
 "highlight" : {
        "pre_tags" : ["<strong>", "<b>"],
        "post_tags" : ["</strong >", "</b>"],
        "fields" : {
            "content" : {}
        }
    }
}'

```

#### should

查询必须满足标 title or author 两个条件

```
GET /_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "title":  "Linux" }},
        { "match": { "author": "Neo"   }}
      ]
    }
  }
}

```

可以嵌套使用

```
GET /_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "title":  "War and Peace" }},
        { "match": { "author": "Leo Tolstoy"   }},
        { "bool":  {
          "should": [
            { "match": { "translator": "Constance Garnett" }},
            { "match": { "translator": "Louise Maude"      }}
          ]
        }}
      ]
    }
  }
}				

```

#### must_not

### filter 过滤

query 相当于 SQL 中的 LIKE 匹配， filter 更像是 where 条件。下面的例子查询 site_id = 23 的数据并且 tags 包含 “头条” 关键字

```
curl -XGET 'http://test:123456@so.netkiller.cn/information/article/_search?pretty' -d '
{
  "query": {
    "bool": {
      "must": {
        "match": {
          "tags": "头条"
        }
      },
      "filter": {
        "term": {
          "site_id" : "23"
        }
      }
    }
  }
}'

```

### sort 排序

```
curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"query" : {
		"match" : {"tag" : "美"}
	},
	"sort": {
		"ctime": {"order": "desc", "mode":  "min"}
	}
}
'			
```

### _source

```
curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"query" : {
		"match" : {
			"tag" : "美"   
		}
	},
	"_source":["id","title","ctime"]
}
'

curl -XGET 'http://localhost:9200/information/news/_search?pretty' -d '
{
	"_source":["id","title","ctime"],
	"query" : {
		"match" : {"tag" : "美"}
	},
	"sort": {
		"ctime": {"order": "desc", "mode":  "min"}
	}
}
'	

```

### highlight 高亮处理

```

curl -XPOST http://test:123456@so.netkiller.cn/information/article/_search  -d'
{
    "query" : { "match" : {  "content" : "股市" }},
    "highlight" : {
        "pre_tags" : ["<strong>", "<b>"],
        "post_tags" : ["</strong >", "</b>"],
        "fields" : {
            "content" : {}
        }
    }
}
'			

```

## 集群管理

查看节点信息

```

root@netkiller /var/log/elasticsearch % curl -XGET localhost:9200/
{
  "name" : "node-1",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "I7jirJ2yTr-f2qrBNorQYA",
  "version" : {
    "number" : "5.6.0",
    "build_hash" : "781a835",
    "build_date" : "2017-09-07T03:09:58.087Z",
    "build_snapshot" : false,
    "lucene_version" : "6.6.0"
  },
  "tagline" : "You Know, for Search"
}

```

### 节点健康状态

```
root@netkiller ~ % curl 'http://localhost:9200/_cat/health?v' 
epoch      timestamp cluster        status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1502445967 18:06:07  my-application yellow          2         2     17  11    0    0        5             0                  -                 77.3%			

```

```
root@netkiller ~ % curl 'http://localhost:9200/_cluster/health'  
{"cluster_name":"my-application","status":"yellow","timed_out":false,"number_of_nodes":2,"number_of_data_nodes":2,"active_primary_shards":11,"active_shards":17,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":5,"delayed_unassigned_shards":0,"number_of_pending_tasks":0,"number_of_in_flight_fetch":0,"task_max_waiting_in_queue_millis":0,"active_shards_percent_as_number":77.27272727272727} 			

```

### 节点 http 状态

```
root@netkiller ~ % curl 'localhost:9200/_nodes/stats/http?pretty'
{
  "_nodes" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "cluster_name" : "my-application",
  "nodes" : {
    "-lnKCmBXRpiwExLns0jc9g" : {
      "timestamp" : 1502446878773,
      "name" : "node-1",
      "transport_address" : "10.104.3.2:9300",
      "host" : "10.104.3.2",
      "ip" : "10.104.3.2:9300",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "http" : {
        "current_open" : 4,
        "total_opened" : 29
      }
    },
    "WVsgYi2HT8GWnZU1kUwFwA" : {
      "timestamp" : 1502446878782,
      "name" : "node-2",
      "transport_address" : "10.186.7.221:9300",
      "host" : "10.186.7.221",
      "ip" : "10.186.7.221:9300",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "http" : {
        "current_open" : 0,
        "total_opened" : 2
      }
    }
  }
}

```

### 查看 master 节点

```
root@netkiller ~ % curl 'http://localhost:9200/_cat/nodes?v'      
ip           heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
10.104.3.2             31          98  50    0.20    0.55     0.61 mdi       -      node-1
10.186.7.221           25          99   0    0.00    0.01     0.05 mdi       *      node-2

```

### 查看索引的节点分布

```
root@netkiller ~ % curl 'http://localhost:9200/_cat/shards?v'
index        shard prirep state      docs   store ip           node
logstash-api 1     p      STARTED     103   342kb 10.186.7.221 node-2
logstash-api 1     r      STARTED     103   342kb 10.104.3.2   node-1
logstash-api 3     p      STARTED     104 404.1kb 10.186.7.221 node-2
logstash-api 3     r      STARTED     104 404.1kb 10.104.3.2   node-1
logstash-api 4     p      STARTED     117 349.2kb 10.186.7.221 node-2
logstash-api 4     r      STARTED     117 349.2kb 10.104.3.2   node-1
logstash-api 2     p      STARTED     113 405.8kb 10.186.7.221 node-2
logstash-api 2     r      STARTED     113 405.8kb 10.104.3.2   node-1
logstash-api 0     p      STARTED     128 488.4kb 10.186.7.221 node-2
logstash-api 0     r      STARTED     128 488.4kb 10.104.3.2   node-1
.kibana      0     p      STARTED       5  31.2kb 10.186.7.221 node-2
.kibana      0     r      STARTED       5  31.2kb 10.104.3.2   node-1
information  1     p      STARTED      10 122.3kb 10.104.3.2   node-1
information  1     r      UNASSIGNED                           
information  3     p      STARTED       7 159.6kb 10.104.3.2   node-1
information  3     r      UNASSIGNED                           
information  4     p      STARTED       8  86.1kb 10.104.3.2   node-1
information  4     r      UNASSIGNED                           
information  2     p      STARTED      11   160kb 10.104.3.2   node-1
information  2     r      UNASSIGNED                           
information  0     p      STARTED       9 202.8kb 10.104.3.2   node-1
information  0     r      UNASSIGNED                           

```

### 索引的开启与关闭

```
POST /{index}/_close		关闭索引
POST /{index}/_open		打开索引                                                                                                                                                                      

```

#### _open

```
root@netkiller /var/log/elasticsearch % curl 'http://localhost:9200/_cat/indices?v'   
health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size
       close  information     oygxIi-dR1eB9NoIZtJrxQ                                                          
green  open   logstash-spring 9xXtt_L0QvKHibXFH5gjJQ   5   1       4622            0     62.9mb         31.4mb
green  open   .kibana         9jBBaOomTO2EakZlZqnE-g   1   1         10            0     36.6kb         18.3kb				

```

```
root@netkiller /var/log/elasticsearch % curl -XPOST 'http://localhost:9200/information/_open'
{"acknowledged":true}#  				

```

```
root@netkiller /var/log/elasticsearch % curl 'http://localhost:9200/_cat/indices?v'         
health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   information     oygxIi-dR1eB9NoIZtJrxQ   5   1       2417            5     34.5mb         17.3mb
green  open   logstash-spring 9xXtt_L0QvKHibXFH5gjJQ   5   1       4622            0     62.9mb         31.4mb
green  open   .kibana         9jBBaOomTO2EakZlZqnE-g   1   1         10            0     36.6kb         18.3kb					

```

#### _close

```
root@netkiller /var/log/elasticsearch % curl -XPOST 'http://localhost:9200/information/_close'
{"acknowledged":true}#  				

```

## 中文分词插件管理

### 通过 elasticsearch-plugin 命令安装分词插件

```
root@netkiller ~ % /usr/share/elasticsearch/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
-> Downloading https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
[=================================================] 100%   
-> Installed analysis-ik

```

创建 mapping

```
root@netkiller ~ % curl -XPUT http://localhost:9200/information

root@netkiller ~ % curl -XPOST http://localhost:9200/information/article/_mapping -d'
{
        "properties": {
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "title": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            }
        }
}'

root@netkiller ~ % curl "http://localhost:9200/information/article/_mapping?pretty"
{
  "information" : {
    "mappings" : {
      "article" : {
        "properties" : {
          "content" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          },
          "title" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          }
        }
      }
    }
  }
}

```

### 手工安装插件

```
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/elasticsearch-analysis-ik-5.5.0.sh | bash

```

### 创建索引

```
curl -XPUT http://localhost:9200/information

```

### 删除索引

如果索引已经存在请删除后重新创建索引

```
curl -XDELETE http://localhost:9200/information/news/_mapping?pretty
curl -XDELETE http://localhost:9200/information/?pretty			

```

### 配置索引分词插件

```

curl -XPOST http://localhost:9200/information/news/_mapping?pretty -d'
{
    "news": {
            "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        },
        "properties": {
            "content": {
                "type": "text",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            }
        }
    }
}'

```

#### 测试分词效果

```

curl -XPOST http://localhost:9200/information/news/ -d'
{"title": "越南胡志明游记·教堂·管风琴的天籁之音","content":"这是我平生第一次去教堂，也是第一次完整的参加宗教仪式。当我驻足教堂外的时候，耳边传来天籁之音，是管风琴，确切的说是电子风琴。真正的管风琴造价昂贵，管风琴通常需要根据教堂尺寸定制，无法量产。我记得中国只有 4 座管风琴，深圳音乐厅有一座。"}
'
curl -XPOST http://localhost:9200/information/news/ -d'
{"title": "越南胡志明游记·信仰·法事","content":"佛经的形成过程是与佛教的发展相始终的，按照佛教发展的时间顺序，最早形成的是小乘佛教三藏，之后形成的是大乘佛教三藏，最后形成的是密宗三藏。"}
'

curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "content" : "佛经" }},
    "highlight" : {
        "pre_tags" : ["<strong>", "<strong>"],
        "post_tags" : ["</strong>", "</strong>"],
        "fields" : {
            "content" : {}
        }
    }
}'		

curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "content" : "中国" }},
    "highlight" : {
        "pre_tags" : ["<b>", "<i>"],
        "post_tags" : ["</b>", "</i>"],
        "fields" : {
            "content" : {}
        }
    }
}'					

```

## 第 8 章 Elasticsearch

http://www.elasticsearch.org/

## 安装 Elasticsearch

### 6.x 安装

安装 6.x 仓库

```

curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elastic/elastic-6.x.sh | bash			

```

安装 6.x 包

```

yum install elasticsearch			

```

### 单机模式 (适用于开发环境) 5.x

使用 Netkiller OSCM 一键安装 Elasticsearch 5.6.0

```
# Java
curl -s https://raw.githubusercontent.com/oscm/shell/master/lang/java/openjdk/java-1.8.0-openjdk.sh | bash

# Install
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/elasticsearch-5.x.sh | bash

# Bind 0.0.0.0
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/network.bind_host.sh | bash

# Auto create index
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/action.auto_create_index.sh | bash

# elasticsearch-analysis-ik

curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/5.5/elasticsearch-analysis-ik-5.6.0.sh | bash

```

通常 elasticsearch-analysis-ik 的版本会比 elasticsearch 慢一个版本，所以请使用下面命令查看版本是否一致，如果不一致可以修改 plugin-descriptor.properties 配置文件，使其一致。

```
root@netkiller /usr/share/elasticsearch/plugins/ik % grep ^version plugin-descriptor.properties
version=5.5.1

```

启动后使用 jps 命令检查进城是否工作正常

```
root@netkiller /var/log/elasticsearch % jps | grep Elasticsearch
9706 Elasticsearch

root@netkiller /var/log/elasticsearch % ss -lnt | grep 9200
LISTEN     0      128    127.0.0.1:9200                     *:*

```

### Elasticsearch Cluster 5.x

集群模式需要两个以上的节点，通常是一个 master 节点，多个 data 节点

首先在所有节点上安装 elasticsearch，然后配置各节点的配置文件，对于 5.5.1 不需要配置决定哪些节点属于 master 节点 或者 data 节点。

```
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/elasticsearch/elasticsearch-5.x.sh | bash			

```

配置文件

```
cluster.name: elasticsearch-cluster # 配置集群名称,所有服务器服务器保持一致

node.name: node-1 # 每个节点唯一标识，每个节点只需改动这里，一次递增 node-1, node-2, node-3 ...

network.host: 0.0.0.0

discovery.zen.ping.unicast.hosts: ["172.16.0.20", "172.16.0.21","172.16.0.22"]  # 所有节点的 IP 地址写在这里

discovery.zen.minimum_master_nodes: 3 # 可以作为 master 的节点总数，有多少个节点就写多少

http.cors.enabled: true
http.cors.allow-origin: "*"

```

查看节点状态，使用 curl 工具: curl 'http://localhost:9200/_nodes/process?pretty'

```
root@netkiller /var/log/elasticsearch % curl 'http://localhost:9200/_nodes/process?pretty'
{
  "_nodes" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "cluster_name" : "my-application",
  "nodes" : {
    "-lnKCmBXRpiwExLns0jc9g" : {
      "name" : "node-1",
      "transport_address" : "10.104.3.2:9300",
      "host" : "10.104.3.2",
      "ip" : "10.104.3.2",
      "version" : "5.5.1",
      "build_hash" : "19c13d0",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 23669,
        "mlockall" : false
      }
    },
    "WVsgYi2HT8GWnZU1kUwFwA" : {
      "name" : "node-2",
      "transport_address" : "10.186.7.221:9300",
      "host" : "10.186.7.221",
      "ip" : "10.186.7.221",
      "version" : "5.5.1",
      "build_hash" : "19c13d0",
      "roles" : [
        "master",
        "data",
        "ingest"
      ],
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 12641,
        "mlockall" : false
      }
    }
  }
}

```

启动节点后回生成 cluster.name 为文件名的日志文件。

谁先启动谁讲成为 master

```
[2017-08-11T17:42:46,018][INFO ][o.e.c.s.ClusterService   ] [node-1] new_master {node-1}{-lnKCmBXRpiwExLns0jc9g}{rZcJDIynSzq2Td3yP2kN5A}{10.104.3.2}{10.104.3.2:9300}, added {{node-2}{WVsgYi2HT8GWnZU1kUwFwA}{X13ShUpAQa2zA1Mgcsm3bQ}{10.186.7.221}{10.186.7.221:9300},}, reason: zen-disco-elected-as-master ([1] nodes joined)[{node-2}{WVsgYi2HT8GWnZU1kUwFwA}{X13ShUpAQa2zA1Mgcsm3bQ}{10.186.7.221}{10.186.7.221:9300}]			

```

如果 master 出现故障，其他节点会接管

```
[2017-08-11T17:44:52,797][INFO ][o.e.c.s.ClusterService   ] [node-2] master {new {node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300}}, removed {{node-1}{-lnKCmBXRpiwExLns0jc9g}{rZcJDIynSzq2Td3yP2kN5A}{10.104.3.2}{10.104.3.2:9300},}, added {{node-1}{-lnKCmBXRpiwExLns0jc9g}{odnoG9kpQpeX1ltx5KYTSw}{10.104.3.2}{10.104.3.2:9300},}, reason: zen-disco-elected-as-master ([1] nodes joined)[{node-1}{-lnKCmBXRpiwExLns0jc9g}{odnoG9kpQpeX1ltx5KYTSw}{10.104.3.2}{10.104.3.2:9300}]
[2017-08-11T17:44:53,184][INFO ][o.e.c.r.DelayedAllocationService] [node-2] scheduling reroute for delayed shards in [59.5s] (11 delayed shards)
[2017-08-11T17:44:53,929][INFO ][o.e.c.r.a.AllocationService] [node-2] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[information][0]] ...]).		

```

master 节点恢复上线会提示

```
[2017-08-11T17:44:52,855][INFO ][o.e.c.s.ClusterService   ] [node-1] detected_master {node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300}, added {{node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300},}, reason: zen-disco-receive(from master [master {node-2}{WVsgYi2HT8GWnZU1kUwFwA}{vl8kQx8sQdGVVohrNQnZOQ}{10.186.7.221}{10.186.7.221:9300} committed version [44]])

```

### 负载均衡配置

首先安装 nginx, 这里使用 Netkiller OSCM 一键安装脚本完成。

```
# curl -s https://raw.githubusercontent.com/oscm/shell/master/web/nginx/stable/nginx.sh | bash

```

因为 elasticsearch 没有用户认证机制我们通常在内网访问他。如果对外提供服务需要增加用户认证。

```

# printf "neo:$(openssl passwd -crypt s3cr3t)n" > /etc/nginx/passwords 			

```

创建 nginx 配置文件 /etc/nginx/conf.d/elasticsearch.conf

```
upstream elasticsearch {
	server 172.16.0.10:9200;
	server 172.16.0.20:9200;
	server 172.16.0.30:9200;

	keepalive 15;
}

server {
	listen 9200;
	server_name so.netkiller.cn;

	charset utf-8;
    access_log /var/log/nginx/so.netkiller.cn.access.log;
    error_log /var/log/nginx/so.netkiller.cn.error.log;

	auth_basic "Protected Elasticsearch";
	auth_basic_user_file passwords;

	location ~* ^(/_cluster|/_nodes) {
		return 403;
		break;
	}
    location ~* _(open|close) {
            return 403;
            break;
    }
	location / {

		if ($request_filename ~ _shutdown) {
		    return 403;
		    break;
		}

        if ($request_method !~ ^(GET|HEAD|POST)$) {
			return 403;
		}

		proxy_pass http://elasticsearch;
		proxy_http_version 1.1;
		proxy_set_header Connection "Keep-Alive";
		proxy_set_header Proxy-Connection "Keep-Alive";
	}

}

```

反复使用下面方法请求，最终你会发现 total_opened 会达到你的 nginx 配置数量

```
$ curl 'http://test:test@localhost:9200/_nodes/stats/http?pretty' | grep total_opened
# "total_opened" : 15			

```

上面的例子适用于绝大多数场景。

例 8.1. Elasticsearch master / slave

```

upstream elasticsearch {
	server 172.16.0.10:9200;
	server 172.16.0.20:9200 backup;

	keepalive 15;
}

server {
	listen 9200;
	server_name so.netkiller.cn;

	auth_basic "Protected Elasticsearch";
	auth_basic_user_file passwords;

	location ~* ^(/_cluster|/_nodes) {
		return 403;
		break;
	} 

	location / {

		if ($request_filename ~ _shutdown) {
		    return 403;
		    break;
		}
		if ($request_method !~ "HEAD") {
          return 403;
          break;
        }
        if ($request_method ~ "DELETE") {
          return 403;
          break;
        }

		proxy_pass http://elasticsearch;
		proxy_http_version 1.1;
		proxy_set_header Connection "Keep-Alive";
		proxy_set_header Proxy-Connection "Keep-Alive";
	}

}

```

通过 limit_except 可以控制访问权限，例如删除操作。

```

limit_except PUT {
	allow 192.168.1.1;
	deny all;
}
limit_except DELETE {
	allow 192.168.1.1;
	deny all;
}

```

### 安装指定版本的 Elasticsearch

使用 yum 安装默认为最新版本，我们常常会遇到一个问题 elasticsearch-analysis-ik 的版本晚于 Elasticsearch。如果使用 yum 安装 Elasticsearch 可能 elasticsearch-analysis-ik 插件不支持这个版本，有些版本的 elasticsearch-analysis-ik 可以修改插件配置文件中的版本号，使其与 elasticsearch 版本相同，可以欺骗 elasticsearch 跳过版本不一致异常。

最佳的解决方案是去 [elasticsearch-analysis-ik github](https://github.com/medcl/elasticsearch-analysis-ik) 找到兼容的版本，安装我们安装 elasticsearch-analysis-ik 的版本需求来指定安装 elasticsearch

```
Versions

IK version	ES version
master	5.x -> master
5.6.0	5.6.0
5.5.3	5.5.3
5.4.3	5.4.3
5.3.3	5.3.3
5.2.2	5.2.2
5.1.2	5.1.2
1.10.1	2.4.1
1.9.5	2.3.5
1.8.1	2.2.1
1.7.0	2.1.1
1.5.0	2.0.0
1.2.6	1.0.0
1.2.5	0.90.x
1.1.3	0.20.x
1.0.0	0.16.2 -> 0.19.0			

```

最新版是 elasticsearch 5.6.1 但分词插件 elasticsearch-analysis-ik 仅能支持到 elasticsearch 版本是 5.6.0

```
root@netkiller /var/log % yum --showduplicates list elasticsearch | expand | tail
Repository epel is listed more than once in the configuration  
elasticsearch.noarch                 5.5.3-1                  elasticsearch-5.x     
elasticsearch.noarch                 5.6.0-1                  elasticsearch-5.x   
elasticsearch.noarch                 5.6.1-1                  elasticsearch-5.x 

```

安装 5.6.0

```
# yum install elasticsearch-5.6.0-1

Loaded plugins: fastestmirror, langpacks
Repository epel is listed more than once in the configuration
Loading mirror speeds from cached hostfile
Resolving Dependencies
--> Running transaction check
---> Package elasticsearch.noarch 0:5.6.0-1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==========================================================================================================================================================================================================
 Package                                            Arch                                        Version                                      Repository                                              Size
==========================================================================================================================================================================================================
Installing:
 elasticsearch                                      noarch                                      5.6.0-1                                      elasticsearch-5.x                                       32 M

Transaction Summary
==========================================================================================================================================================================================================
Install  1 Package

Total download size: 32 M
Installed size: 36 M
Is this ok [y/d/N]: y

```

### Plugin

Elasticsearch 提供了插件管理命令 elasticsearch-plugin

```
root@netkiller ~ % /usr/share/elasticsearch/bin/elasticsearch-plugin -h
A tool for managing installed elasticsearch plugins

Commands
--------
list - Lists installed elasticsearch plugins
install - Install a plugin
remove - removes a plugin from Elasticsearch

Non-option arguments:
command              

Option         Description        
------         -----------        
-h, --help     show help          
-s, --silent   show minimal output
-v, --verbose  show verbose output			

```

#### elasticsearch-analysis-ik

安装插件

```
root@netkiller ~ % /usr/share/elasticsearch/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
-> Downloading https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
[=================================================] 100%   
-> Installed analysis-ik

```

```
curl -XPOST http://localhost:9200/index/fulltext/_mapping -d'
{
        "properties": {
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            }
        }

}'			

```

#### elasticsearch-analysis-pinyin

https://github.com/medcl/elasticsearch-analysis-pinyin

## 映射

### 查看 _mapping

```
curl -XGET http://localhost:9200/information/news/_mapping?pretty		

```

数据结构如下

```
{
  "information" : {
    "mappings" : {
      "news" : {
        "_all" : {
          "analyzer" : "ik_max_word"
        },
        "properties" : {
          "content" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "ctime" : {
            "type" : "string"
          },
          "division_category_id" : {
            "type" : "long"
          },
          "tag" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "title" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          }
        }
      }
    }
  }
}

```

### 删除 _mapping

```
curl -XDELETE http://localhost:9200/information/news/_mapping?pretty		

```

### 创建 _mapping

```
curl -XPOST http://localhost:9200/information/news/_mapping?pretty -d'
{
    "news": {
        "_all": {
	       "analyzer": "ik_max_word",
	       "search_analyzer": "ik_max_word",
	       "term_vector": "no",
	       "store": "false"
        },
        "properties": {
            "content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            }
        }
    }
}'

```

### 更新 mapping

注意：更新只能用于空的 index，如果 index 中存在数据无法修改 _mapping，必须重建，或者采用别名方案

更新已存在的 mapping，首先我们创建一个 _mapping

```

% curl "http://localhost:9200/information/article/_mapping?pretty"
{
  "information" : {
    "mappings" : {
      "article" : {
        "properties" : {
          "content" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          },
          "title" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          }
        }
      }
    }
  }
}

```

在这个 _mapping 中增加 ctime 字段，定义时间格式为 yyyy-MM-dd HH:mm:ss

```

% curl -XPOST http://localhost:9200/information/article/_mapping -d'
{
        "properties": {
        "ctime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           }
        }
}'

```

查看预期结果

```

% curl "http://localhost:9200/information/article/_mapping?pretty"  
{
  "information" : {
    "mappings" : {
      "article" : {
        "properties" : {
          "content" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          },
          "ctime" : {
            "type" : "date",
            "format" : "yyyy-MM-dd HH:mm:ss"
          },
          "title" : {
            "type" : "text",
            "analyzer" : "ik_max_word"
          }
        }
      }
    }
  }
}

```

### 修改 _mapping

修改流程需要经历五步，首先创建新索引，创建新 _mapping，导入数据，索引别名，删除旧索引。

当然你也可以删除重建索引，为什么会这么折腾呢？因为这样不用停止业务的情况下进行迁移。

```

# curl -XGET http://localhost:9200/information_v1/news/_mapping?pretty
{
  "information_v1" : {
    "mappings" : {
      "news" : {
        "_all" : {
          "analyzer" : "ik_max_word"
        },
        "properties" : {
          "content" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "ctime" : {
            "type" : "string"
          },
          "division_category_id" : {
            "type" : "long"
          },
          "tag" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "title" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          }
        }
      }
    }
  }
}

```

注意 ctime 数据类型定义错误，现在需要将它改为 date 日期类型。

创建 information_v2 索引

```

curl -XPUT http://localhost:9200/information_v2
curl -XPOST http://localhost:9200/information_v2/news/_mapping?pretty -d'
{
    "news": {
            "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        	},
		"properties": {
			"title": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"tag": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            },
            "ctime": { 
				"type": "date"
		    }
        }
    }
}'	

```

查看全新 _mapping

```

# curl -XGET http://localhost:9200/information_v2/news/_mapping?pretty
{
  "information_v2" : {
    "mappings" : {
      "news" : {
        "_all" : {
          "analyzer" : "ik_max_word"
        },
        "properties" : {
          "content" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "ctime" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "tag" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "title" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          }
        }
      }
    }
  }
}

```

现在导入数据，导入完成后修改别名，将 information 从 information_v1 切换到 information_v2

```

curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "remove": {
            "alias": "information",
            "index": "information_v1"
        }},
        { "add": {
            "alias": "information",
            "index": "information_v2"
        }}
    ]
}
'

```

当所以切换完成 information_v1 已经没有什么用处了，这时可以删除 information_v1

```

curl -XDELETE http://localhost:9200/information_v1

```

### 数据类型

string, date, long, double, boolean or ip.

#### date

elasticsearch 采用 ISO 8601 标准的 date 格式

```

{"LastUpdate": {
    "type" : "date",
    "format" : "yyyy-MM-dd HH:mm:ss"}
}				

```

```

{
  "mappings": {
    "my_type": {
      "properties": {
        "date": {
          "type":   "date",
          "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
        }
      }
    }
  }
}				

```

## Alias management 别名管理

### 查看索引别名

没有设置任何别名将返回下面的数据结构

```
# curl -XGET http://localhost:9200/_aliases?pretty
{
  "information_v1" : {
    "aliases" : { }
  },
  "information_v2" : {
    "aliases" : { }
  }
}

```

information 是 information_v1 的别名

```
# curl -XGET http://localhost:9200/_aliases?pretty
{
  "information_v1" : {
    "aliases" : {
      "information" : { }
    }
  },
  "information_v2" : {
    "aliases" : { }
  }
}

```

### 创建索引别名

```
curl -XPUT http://localhost:9200/information_v1
curl -XPOST http://localhost:9200/information_v1/news/_mapping?pretty -d'
{
    "news": {
            "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        	},
		"properties": {
			"title": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"tag": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            },
            "ctime": { 
				"type": "date"
		    }
        }
    }
}'	

```

```
curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "add": {
            "alias": "information",
            "index": "information_v1"
        }}
    ]
}
'

{"acknowledged":true}

```

查看结果

```
# curl -XGET http://localhost:9200/_aliases?pretty
{
  "information_v1" : {
    "aliases" : {
      "information" : { }
    }
  },
  "information_v2" : {
    "aliases" : { }
  }
}

# curl -XGET http://localhost:9200/information/?pretty
{
  "information_v1" : {
    "aliases" : {
      "information" : { }
    },
    "mappings" : {
      "news" : {
        "_all" : {
          "analyzer" : "ik_max_word"
        },
        "properties" : {
          "content" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "ctime" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "tag" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          },
          "title" : {
            "type" : "string",
            "boost" : 8.0,
            "term_vector" : "with_positions_offsets",
            "analyzer" : "ik_max_word",
            "include_in_all" : true
          }
        }
      }
    },
    "settings" : {
      "index" : {
        "creation_date" : "1471929807430",
        "number_of_shards" : "5",
        "number_of_replicas" : "1",
        "uuid" : "gWl8TTT-QnKbKj2BglfG-w",
        "version" : {
          "created" : "2030599"
        }
      }
    },
    "warmers" : { }
  }
}

```

### 修改别名

```

curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "remove": {
            "alias": "information",
            "index": "information_v1"
        }},
        { "add": {
            "alias": "information",
            "index": "information_v2"
        }}
    ]
}
'

```

### 删除别名

```

curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "remove": {
            "alias": "information","index": "information_v2"
        }}
    ]
}
'

```

## Example

### 新闻资讯应用案例

```

curl -XDELETE http://localhost:9200/information_v1/news/_mapping?pretty
curl -XDELETE http://localhost:9200/information_v1/?pretty

curl -XPUT http://localhost:9200/information_v1

curl -XPOST http://localhost:9200/information_v1/news/_mapping?pretty -d'
{
    "news": {
            "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        	},
		"properties": {
			"id": { 
				"type": "long"
		    },
			"title": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
			},
			"tag": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            },
            "ctime": { 
				"type": "date"
		    }
        }
    }
}'

curl -XPOST http://localhost:9200/_aliases -d '
{
    "actions": [
        { "add": {
            "alias": "information",
            "index": "information_v1"
        }}
    ]
}
'

curl -XGET http://localhost:9200/information/?pretty

curl -XPOST 'http://localhost:9200/information/news/1?pretty' -d '{
	"id":1,
	"title":"新闻标题",
	"content":"新闻内容",
	"tag":"新闻标签",
	"ctime":"2011-11-11T11:11:11"
}'

# curl -XGET 'http://localhost:9200/information/news/1?pretty'
{
  "_index" : "information_v1",
  "_type" : "news",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "id" : 1,
    "title" : "新闻标题",
    "content" : "新闻内容",
    "tag" : "新闻标签",
    "ctime" : "2011-11-11T11:11:11"
  }
}

curl -XPOST http://localhost:9200/information/news/_search?pretty  -d'
{
    "query" : { "term" : { "content" : "新闻" }},
    "highlight" : {
        "pre_tags" : ["<b>", "<b>"],
        "post_tags" : ["</b>", "</b>"],
        "fields" : {
            "content" : {}
        }
    }
}'

curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "content" : "王宝强" }},
    "highlight" : {
        "pre_tags" : ["<b>", "<b>"],
        "post_tags" : ["</b>", "</b>"],
        "fields" : {
            "content" : {}
        }
    }
}'

curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "tag" : "娱乐" }},
    "highlight" : {
        "pre_tags" : ["<b>", "<b>"],
        "post_tags" : ["</b>", "</b>"],
        "fields" : {
            "tag" : {}
        }
    }
}'

```

### 文章搜索案例

```

curl -XDELETE http://localhost:9200/information

curl -XPUT http://localhost:9200/information

curl -H 'Content-Type: application/json' -XPOST http://localhost:9200/information/article/_mapping -d'
{
        "properties": {
            "title": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "description": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "ctime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           	},
 			"mtime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           	}
        }
}'

curl "http://localhost:9200/information/article/_mapping?pretty"

```

## Migrating MySQL Data into Elasticsearch using logstash

https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html

### 安装 logstash

安装 JDBC 驱动 和 Logstash

```
curl -s https://raw.githubusercontent.com/oscm/shell/master/database/mysql/5.7/mysql-connector-java.sh	 | bash			
curl -s https://raw.githubusercontent.com/oscm/shell/master/search/logstash/logstash-5.x.sh | bash

```

mysql 驱动文件位置在 /usr/share/java/mysql-connector-java.jar

### 配置 logstash

创建配置文件 /etc/logstash/conf.d/jdbc-mysql.conf

```

mysql> desc article;
+-------------+--------------+------+-----+---------+-------+
| Field       | Type         | Null | Key | Default | Extra |
+-------------+--------------+------+-----+---------+-------+
| id          | int(11)      | NO   |     | 0       |       |
| title       | mediumtext   | NO   |     | NULL    |       |
| description | mediumtext   | YES  |     | NULL    |       |
| author      | varchar(100) | YES  |     | NULL    |       |
| source      | varchar(100) | YES  |     | NULL    |       |
| ctime       | datetime     | NO   |     | NULL    |       |
| content     | longtext     | YES  |     | NULL    |       |
+-------------+--------------+------+-----+---------+-------+
7 rows in set (0.00 sec)

```

```

input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"
    statement => "select * from article"
  }
}
output {
    elasticsearch {
    		hosts => "localhost:9200"
        index => "information"
        document_type => "article"
        document_id => "%{id}"

    }
}

```

### 启动 Logstash

```

root@netkiller /var/log/logstash % systemctl restart logstash

root@netkiller /var/log/logstash % systemctl status logstash
● logstash.service - logstash
   Loaded: loaded (/etc/systemd/system/logstash.service; enabled; vendor preset: disabled)
   Active: active (running) since Mon 2017-07-31 09:35:00 CST; 11s ago
 Main PID: 10434 (java)
   CGroup: /system.slice/logstash.service
           └─10434 /usr/bin/java -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+DisableExplicitGC -Djava.awt.headless=true -Dfi...

Jul 31 09:35:00 netkiller systemd[1]: Started logstash.
Jul 31 09:35:00 netkiller systemd[1]: Starting logstash...

root@netkiller /var/log/logstash % cat logstash-plain.log 
[2017-07-31T09:35:28,169][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2017-07-31T09:35:28,172][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2017-07-31T09:35:28,298][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>#<Java::JavaNet::URI:0x453a18e9>}
[2017-07-31T09:35:28,299][INFO ][logstash.outputs.elasticsearch] Using mapping template from {:path=>nil}
[2017-07-31T09:35:28,337][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{"template"=>"logstash-*", "version"=>50001, "settings"=>{"index.refresh_interval"=>"5s"}, "mappings"=>{"_default_"=>{"_all"=>{"enabled"=>true, "norms"=>false}, "dynamic_templates"=>[{"message_field"=>{"path_match"=>"message", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false}}}, {"string_fields"=>{"match"=>"*", "match_mapping_type"=>"string", "mapping"=>{"type"=>"text", "norms"=>false, "fields"=>{"keyword"=>{"type"=>"keyword", "ignore_above"=>256}}}}}], "properties"=>{"@timestamp"=>{"type"=>"date", "include_in_all"=>false}, "@version"=>{"type"=>"keyword", "include_in_all"=>false}, "geoip"=>{"dynamic"=>true, "properties"=>{"ip"=>{"type"=>"ip"}, "location"=>{"type"=>"geo_point"}, "latitude"=>{"type"=>"half_float"}, "longitude"=>{"type"=>"half_float"}}}}}}}}
[2017-07-31T09:35:28,344][INFO ][logstash.outputs.elasticsearch] Installing elasticsearch template to _template/logstash
[2017-07-31T09:35:28,465][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>[#<Java::JavaNet::URI:0x66df34ae>]}
[2017-07-31T09:35:28,483][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>1000}
[2017-07-31T09:35:29,562][INFO ][logstash.pipeline        ] Pipeline main started
[2017-07-31T09:35:29,700][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2017-07-31T09:36:01,019][INFO ][logstash.inputs.jdbc     ] (0.006000s) select * from article	

```

### 验证

```

% curl -XGET 'http://localhost:9200/_all/_search?pretty'

```

### 配置模板

#### 全量导入

适合数据没有改变的归档数据或者只能增加没有修改的数据

```

input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"
    statement => "select * from article"
  }
}
output {
    elasticsearch {
    		hosts => "localhost:9200"
        index => "information"
        document_type => "article"
        document_id => "%{id}"

    }
}

```

#### 多表导入

多张数据表导入到 Elasticsearch

```

# multiple inputs on logstash jdbc

input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"
    statement => "select * from article"
    type => "article"
  }
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"
    statement => "select * from comment"
    type => "comment"
  } 
}
output {
    elasticsearch {
    		hosts => "localhost:9200"
        index => "information"
        document_type => "%{type}"
        document_id => "%{id}"

    }
}				

```

需要在每一个 jdbc 配置项中加入 type 配置，然后 elasticsearch 配置项中加入 document_type => "%{type}"

#### 通过 ID 主键字段增量复制数据

```

input {
  jdbc {
    statement => "SELECT id, mycolumn1, mycolumn2 FROM my_table WHERE id > :sql_last_value"
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric"
    # ... other configuration bits
  }
}

```

tracking_column_type => "numeric" 可以声明 id 字段的数据类型, 如果不指定将会默认为日期

```
[2017-07-31T11:08:00,193][INFO ][logstash.inputs.jdbc     ] (0.020000s) select * from article where id > '2017-07-31 02:47:00'

```

如果复制不对称可以加入 clean_run => true 配置项，清楚数据

#### 通过日期字段增量复制数据

```

input {
  jdbc {
    statement => "SELECT * FROM my_table WHERE create_date > :sql_last_value"
    use_column_value => true
    tracking_column => "create_date"
    # ... other configuration bits
  }
}

```

如果复制不对称可以加入 clean_run => true 配置项，清楚数据

#### 指定 SQL 文件

statement_filepath 指定 SQL 文件，有时 SQL 太复杂写入 statement 配置项维护部方便，可以将 SQL 写入一个文本文件，然后使用 statement_filepath 配置项引用该文件。

```

input {
    jdbc {
        jdbc_driver_library => "/path/to/driver.jar"
        jdbc_driver_class => "org.postgresql.Driver"
        jdbc_url => "jdbc://postgresql"
        jdbc_user => "neo"
        jdbc_password => "password"
        statement_filepath => "query.sql"
    }
}				

```

#### 参数传递

将需要复制的条件参数写入 parameters 配置项

```

input {
  jdbc {
    jdbc_driver_library => "mysql-connector-java-5.1.36-bin.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/mydb"
    jdbc_user => "mysql"
    parameters => { "favorite_artist" => "Beethoven" }
    schedule => "* * * * *"
    statement => "SELECT * from songs where artist = :favorite_artist"
  }
}				

```

#### 控制返回 JDBC 数据量

```
	jdbc_fetch_size => 1000  #jdbc 获取数据的数量大小
	jdbc_page_size => 1000 #jdbc 一页的大小，
	jdbc_paging_enabled => true  #和 jdbc_page_size 组合，将 statement 的查询分解成多个查询,相当于: SELECT * FROM table LIMIT 1000 OFFSET 4000 				

```

#### 输出到不同的 Elasticsearch 中

通过 if [type]=="news" 执行不同的区块，实现将不同的 type 输出到指定的 index 中。

```
output {
	if [type]=="news" {
	  elasticsearch {
	  	hosts => "node1.netkiller.cn:9200"
		index => "information"
		document_id => "%{id}"
	  }
	}

	if [type]=="comment" {
	  elasticsearch {
		hosts => "node2.netkiller.cn:9200"
		index => "information"
		document_id => "%{id}"
	  }
	}
}		

```

#### 日期格式转换

日期格式化, 将 ISO 8601 日期格式转换为 %Y-%m-%d %H:%M:%S

```

input {
	jdbc {
		jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
		jdbc_driver_class => "com.mysql.jdbc.Driver"
		jdbc_connection_string => "jdbc:mysql://127.0.0.1:3306/cms"
		jdbc_user => "cms"
		jdbc_password => "123456"
		schedule => "* * * * *"
		statement => "select * from article limit 5"
	}

}
filter {
	ruby {
		init => "require 'time'"
        code => "event.set('ctime', event.get('ctime').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }

	ruby {
		init => "require 'time'"
        code => "event.set('mtime', event.get('mtime').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }
}
output {

	stdout {
		codec => rubydebug
	}

}				

```

#### example

下面的例子实现了新数据复制，旧数据更新

```

input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"	#定时 cron 的表达式,这里是每分钟执行一次
    statement => "select id, title, description, author, source, ctime, content from article where id > :sql_last_value"
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article.last"
  }
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"	#定时 cron 的表达式,这里是每分钟执行一次
    statement => "select * from article where ctime > :sql_last_value"
    use_column_value => true
    tracking_column => "ctime"
    tracking_column_type => "timestamp" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article-ctime.last"
  }
}
output {
    elasticsearch {
    		hosts => "localhost:9200"
        index => "information"
        document_type => "article"
        document_id => "%{id}"
        action => "update"　 # 操作执行的动作,可选值有["index", "delete", "create", "update"]
        doc_as_upsert => true  #支持 update 模式
    }
}

```

### 解决数据不对称问题

jdbc-input-plugin 只能实现数据库的追加，对于 elasticsearch 增量写入，但经常 jdbc 源一端的数据库可能会做数据库删除或者更新操作。这样一来数据库与搜索引擎的数据库就出现了不对称的情况。

当然你如果有开发团队可以写程序在删除或者更新的时候同步对搜索引擎操作。如果你没有这个能力，可以尝试下面的方法。

这里有一个数据表 article , mtime 字段定义了 ON UPDATE CURRENT_TIMESTAMP 所以每次更新 mtime 的时间都会变化

```

mysql> desc article;
+-------------+--------------+------+-----+--------------------------------+-------+
| Field       | Type         | Null | Key | Default                        | Extra |
+-------------+--------------+------+-----+--------------------------------+-------+
| id          | int(11)      | NO   |     | 0                              |       |
| title       | mediumtext   | NO   |     | NULL                           |       |
| description | mediumtext   | YES  |     | NULL                           |       |
| author      | varchar(100) | YES  |     | NULL                           |       |
| source      | varchar(100) | YES  |     | NULL                           |       |
| content     | longtext     | YES  |     | NULL                           |       |
| status      | enum('Y','N')| NO   |     | 'N'                            |       |
| ctime       | timestamp    | NO   |     | CURRENT_TIMESTAMP              |       |
| mtime       | timestamp    | YES  |     | ON UPDATE CURRENT_TIMESTAMP    |       |
+-------------+--------------+------+-----+--------------------------------+-------+
7 rows in set (0.00 sec)

```

logstash 增加 mtime 的查询规则

```

  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "password"
    schedule => "* * * * *"	#定时 cron 的表达式,这里是每分钟执行一次
    statement => "select * from article where mtime > :sql_last_value"
    use_column_value => true
    tracking_column => "mtime"
    tracking_column_type => "timestamp" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article-mtime.last"
  }

```

创建回收站表，这个事用于解决数据库删除，或者禁用 status = 'N' 这种情况的。

```

CREATE TABLE `elasticsearch_trash` (
  `id` int(11) NOT NULL,
  `ctime` timestamp NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8

```

为 article 表创建触发器

```

CREATE DEFINER=`dba`@`%` TRIGGER `article_BEFORE_UPDATE` BEFORE UPDATE ON `article` FOR EACH ROW
BEGIN
	-- 此处的逻辑是解决文章状态变为 N 的时候，需要将搜索引擎中对应的数据删除。
	IF NEW.status = 'N' THEN
		insert into elasticsearch_trash(id) values(OLD.id);
	END IF;
	-- 此处逻辑是修改状态到 Y 的时候，方式 elasticsearch_trash 仍然存在该文章 ID，导致误删除。所以需要删除回收站中得回收记录。
    IF NEW.status = 'Y' THEN
		delete from elasticsearch_trash where id = OLD.id;
	END IF;
END

CREATE DEFINER=`dba`@`%` TRIGGER `article_BEFORE_DELETE` BEFORE DELETE ON `article` FOR EACH ROW
BEGIN
	-- 此处逻辑是文章被删除同事将改文章放入搜索引擎回收站。
	insert into elasticsearch_trash(id) values(OLD.id);
END

```

接下来我们需要写一个简单地 Shell 每分钟运行一次，从 elasticsearch_trash 数据表中取出数据，然后使用 curl 命令调用 elasticsearch restful 接口，删除被收回的数据。

### 修改 Mapping

<paraf>需求 Elasticsearch 时间格式 从 ISO 8601 到 yyyy-MM-dd HH:mm:ss。首先停止 logstash</paraf>

```

systemctl stop logstash

rm -rf /var/tmp/article* 			

```

修改 /etc/logstash/conf.d/jdbc.conf 配置文件

```

input {
  jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "123456"
    schedule => "* * * * *"
    statement => "select * from article where id > :sql_last_value"
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article.last"
  }
jdbc {
    jdbc_driver_library => "/usr/share/java/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/cms"
    jdbc_user => "cms"
    jdbc_password => "123456"
    schedule => "* * * * *"	#定时 cron 的表达式,这里是每分钟执行一次
    statement => "select * from article where ctime > :sql_last_value"
    use_column_value => true
    tracking_column => "ctime"
    tracking_column_type => "timestamp" 
    record_last_run => true
    last_run_metadata_path => "/var/tmp/article-ctime.last"
  }

}

filter {

    ruby {
        code => "event.set('ctime', event.get('[ctime]').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }

    ruby {
        code => "event.set('mtime', event.get('[mtime]').time.localtime.strftime('%Y-%m-%d %H:%M:%S'))"
    }

}

output {
    elasticsearch {
    	hosts => "localhost:9200"
        index => "information"
        document_type => "article"
        document_id => "%{id}"
        action => "update"
        doc_as_upsert => true
    }
}

```

删除就的 index，重新创建，并配置 mapping。

```

curl -XDELETE http://localhost:9200/information

curl -XPUT http://localhost:9200/information

curl -XPOST http://localhost:9200/information/article/_mapping -d'
{
        "properties": {
            "title": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "description": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word"
            },
            "ctime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           	},
 			"mtime": {
               "type":   "date",
               "format": "yyyy-MM-dd HH:mm:ss"
           	}
        }
}'

curl "http://localhost:9200/information/article/_mapping?pretty"

```

启动 logstash 重新复制数据。

```

rm -rf /var/log/logstash/*
systemctl start logstash			

```

## 安装 Elasticsearch 2.3

### RPM 安装

```
yum localinstall https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/rpm/elasticsearch/2.3.4/elasticsearch-2.3.4.rpm

```

### YUM 安装

```

rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch

cat >> /etc/yum.repos.d/elasticsearch.repo <<EOF
[elasticsearch-2.x]
name=Elasticsearch repository for 2.x packages
baseurl=https://packages.elastic.co/elasticsearch/2.x/centos
gpgcheck=1
gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch
enabled=1
EOF

yum install elasticsearch

sudo /bin/systemctl daemon-reload
sudo /bin/systemctl enable elasticsearch.service

```

查看 RPM 包中所含文件

```
[root@localhost ~]# rpm -ql elasticsearch-2.3.4-1.noarch 
/etc/elasticsearch
/etc/elasticsearch/elasticsearch.yml
/etc/elasticsearch/logging.yml
/etc/elasticsearch/scripts
/etc/init.d/elasticsearch
/etc/sysconfig/elasticsearch
/usr/lib/sysctl.d
/usr/lib/sysctl.d/elasticsearch.conf
/usr/lib/systemd/system/elasticsearch.service
/usr/lib/tmpfiles.d
/usr/lib/tmpfiles.d/elasticsearch.conf
/usr/share/elasticsearch/LICENSE.txt
/usr/share/elasticsearch/NOTICE.txt
/usr/share/elasticsearch/README.textile
/usr/share/elasticsearch/bin
/usr/share/elasticsearch/bin/elasticsearch
/usr/share/elasticsearch/bin/elasticsearch-systemd-pre-exec
/usr/share/elasticsearch/bin/elasticsearch.in.sh
/usr/share/elasticsearch/bin/plugin
/usr/share/elasticsearch/lib
/usr/share/elasticsearch/lib/HdrHistogram-2.1.6.jar
/usr/share/elasticsearch/lib/apache-log4j-extras-1.2.17.jar
/usr/share/elasticsearch/lib/commons-cli-1.3.1.jar
/usr/share/elasticsearch/lib/compiler-0.8.13.jar
/usr/share/elasticsearch/lib/compress-lzf-1.0.2.jar
/usr/share/elasticsearch/lib/elasticsearch-2.3.4.jar
/usr/share/elasticsearch/lib/guava-18.0.jar
/usr/share/elasticsearch/lib/hppc-0.7.1.jar
/usr/share/elasticsearch/lib/jackson-core-2.6.6.jar
/usr/share/elasticsearch/lib/jackson-dataformat-cbor-2.6.6.jar
/usr/share/elasticsearch/lib/jackson-dataformat-smile-2.6.6.jar
/usr/share/elasticsearch/lib/jackson-dataformat-yaml-2.6.6.jar
/usr/share/elasticsearch/lib/jna-4.1.0.jar
/usr/share/elasticsearch/lib/joda-convert-1.2.jar
/usr/share/elasticsearch/lib/joda-time-2.9.4.jar
/usr/share/elasticsearch/lib/jsr166e-1.1.0.jar
/usr/share/elasticsearch/lib/jts-1.13.jar
/usr/share/elasticsearch/lib/log4j-1.2.17.jar
/usr/share/elasticsearch/lib/lucene-analyzers-common-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-backward-codecs-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-core-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-grouping-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-highlighter-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-join-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-memory-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-misc-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-queries-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-queryparser-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-sandbox-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-spatial-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-spatial3d-5.5.0.jar
/usr/share/elasticsearch/lib/lucene-suggest-5.5.0.jar
/usr/share/elasticsearch/lib/netty-3.10.5.Final.jar
/usr/share/elasticsearch/lib/securesm-1.0.jar
/usr/share/elasticsearch/lib/snakeyaml-1.15.jar
/usr/share/elasticsearch/lib/spatial4j-0.5.jar
/usr/share/elasticsearch/lib/t-digest-3.0.jar
/usr/share/elasticsearch/modules
/usr/share/elasticsearch/modules/lang-expression
/usr/share/elasticsearch/modules/lang-expression/antlr4-runtime-4.5.1-1.jar
/usr/share/elasticsearch/modules/lang-expression/asm-5.0.4.jar
/usr/share/elasticsearch/modules/lang-expression/asm-commons-5.0.4.jar
/usr/share/elasticsearch/modules/lang-expression/lang-expression-2.3.4.jar
/usr/share/elasticsearch/modules/lang-expression/lucene-expressions-5.5.0.jar
/usr/share/elasticsearch/modules/lang-expression/plugin-descriptor.properties
/usr/share/elasticsearch/modules/lang-expression/plugin-security.policy
/usr/share/elasticsearch/modules/lang-groovy
/usr/share/elasticsearch/modules/lang-groovy/groovy-2.4.6-indy.jar
/usr/share/elasticsearch/modules/lang-groovy/lang-groovy-2.3.4.jar
/usr/share/elasticsearch/modules/lang-groovy/plugin-descriptor.properties
/usr/share/elasticsearch/modules/lang-groovy/plugin-security.policy
/usr/share/elasticsearch/modules/reindex
/usr/share/elasticsearch/modules/reindex/plugin-descriptor.properties
/usr/share/elasticsearch/modules/reindex/reindex-2.3.4.jar
/usr/share/elasticsearch/plugins
/var/lib/elasticsearch
/var/log/elasticsearch
/var/run/elasticsearch

```

### 测试安装是否正常

启动

```
/etc/init.d/elasticsearch start			

```

链接测试

```
[root@localhost ~]# curl -X GET http://localhost:9200/
{
  "name" : "Jack of Hearts",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.3.4",
    "build_hash" : "e455fd0c13dceca8dbbdbb1665d068ae55dabe3f",
    "build_timestamp" : "2016-06-30T11:24:31Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}

```

### Plugin 插件管理

#### 手工安装插件

```
cd /usr/local/src/
wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v1.9.4/elasticsearch-analysis-ik-1.9.4.zip
cd /usr/share/elasticsearch/plugins
mkdir ik
cd ik
unzip /usr/local/src/elasticsearch-analysis-ik-1.9.4.zip

/etc/init.d/elasticsearch restart

```

#### plugin 命令

```
plugin -install medcl/elasticsearch-analysis-ik/1.9.0 			

```

#### 插件测试

```

curl -XDELETE http://localhost:9200/information/news/_mapping?pretty
curl -XDELETE http://localhost:9200/information/?pretty

curl -XPUT http://localhost:9200/information
curl -XPOST http://localhost:9200/information/news/_mapping?pretty -d'
{
    "news": {
            "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        },
        "properties": {
            "content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            }
        }
    }
}'

curl -XPOST http://localhost:9200/information/news/ -d'
{"title": "越南胡志明游记·教堂·管风琴的天籁之音","content":"这是我平生第一次去教堂，也是第一次完整的参加宗教仪式。当我驻足教堂外的时候，耳边传来天籁之音，是管风琴，确切的说是电子风琴。真正的管风琴造价昂贵，管风琴通常需要根据教堂尺寸定制，无法量产。我记得中国只有 4 座管风琴，深圳音乐厅有一座。"}
'
curl -XPOST http://localhost:9200/information/news/ -d'
{"title": "越南胡志明游记·信仰·法事","content":"佛经的形成过程是与佛教的发展相始终的，按照佛教发展的时间顺序，最早形成的是小乘佛教三藏，之后形成的是大乘佛教三藏，最后形成的是密宗三藏。"}
'

curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "content" : "佛经" }},
    "highlight" : {
        "pre_tags" : ["<strong>", "<strong>"],
        "post_tags" : ["</strong>", "</strong>"],
        "fields" : {
            "content" : {}
        }
    }
}'		

curl -XPOST http://localhost:9200/information/news/_search  -d'
{
    "query" : { "term" : { "content" : "中国" }},
    "highlight" : {
        "pre_tags" : ["<b>", "<i>"],
        "post_tags" : ["</b>", "</i>"],
        "fields" : {
            "content" : {}
        }
    }
}'	

```

## FAQ

### Plugin [analysis-ik] is incompatible with Elasticsearch [2.3.5]. Was designed for version [2.3.4]

```

[2016-08-20 19:18:40,930][INFO ][node                     ] [Morg] version[2.3.5], pid[31494], build[90f439f/2016-07-27T10:36:52Z]
[2016-08-20 19:18:40,930][INFO ][node                     ] [Morg] initializing ...
[2016-08-20 19:18:41,360][ERROR][bootstrap                ] Exception
java.lang.IllegalArgumentException: Plugin [analysis-ik] is incompatible with Elasticsearch [2.3.5]. Was designed for version [2.3.4]
	at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:118)
	at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:378)
	at org.elasticsearch.plugins.PluginsService.<init>(PluginsService.java:128)
	at org.elasticsearch.node.Node.<init>(Node.java:158)
	at org.elasticsearch.node.Node.<init>(Node.java:140)
	at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
	at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
	at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)

```

解决方案

```
cd /usr/share/elasticsearch/plugins/ik
vim plugin-descriptor.properties

elasticsearch.version=2.3.4
改为
elasticsearch.version=2.3.5

```

### plugin [analysis-ik] is incompatible with version [5.6.1]; was designed for version [5.5.2]

解决方案

```
root@netkiller /var/log/elasticsearch % /usr/share/elasticsearch/bin/elasticsearch-plugin list
analysis-ik
WARNING: plugin [analysis-ik] is incompatible with version [5.6.1]; was designed for version [5.5.2]			

```

```
root@netkiller /var/log/elasticsearch % /usr/share/elasticsearch/bin/elasticsearch-plugin remove analysis-ik --purge
-> removing [analysis-ik]...

```

手工安装 5.6.0 然后

```
vim /usr/share/elasticsearch/plugins/analysis-ik/plugin-descriptor.properties 

elasticsearch.version=5.5.2
改为
elasticsearch.version=5.6.1

```

### mapper_parsing_exception: failed to parse [ctime]

date 各位为 YYYY-MM-ddTHH:mm:ss，注意中间的字幕 T

```
{"type":"date","format":"YYYY-MM-dd'T'HH:mm:ss.SSSZ"}

curl -XPOST "http://localhost:9200/netkiller/news/" -d'
{
    "content": "Hello World!",
    "CreateDate": "2009-11-15T12:12:12"
}'

```

### 配置 JAVA_HOME

编辑 /etc/sysconfig/elasticsearch 配置文件

```
# Elasticsearch Java path
JAVA_HOME=/srv/java

```

## 第 9 章 Solr

### *solr-5.3.0*

## 安装

安装

```
yum install -y unzip java-1.8.0-openjdk

wget http://www.us.apache.org/dist/lucene/solr/5.3.0/solr-5.3.0.tgz
tar zxvf solr-5.3.0.tgz
mv solr-5.3.0 /srv/
ln -s /srv/solr-5.3.0/ /srv/solr

adduser -d /srv/solr -c "Apache Solr" solr
chown solr:solr -R /srv/solr-5.3.0

cp /srv/solr-5.3.0/bin/init.d/solr /etc/init.d/
sed -i 's:/opt/solr:/srv/solr:' /etc/init.d/solr
sed -i 's:/var/solr:/srv/solr/bin:' /etc/init.d/solr

chkconfig --add  solr
chkconfig solr on

```

启动与停止

```
# service solr start
Waiting up to 30 seconds to see Solr running on port 8983 [/]
Started Solr server on port 8983 (pid=61909). Happy searching!

# service solr stop
Sending stop command to Solr running on port 8983 ... waiting 5 seconds to allow Jetty process 61909 to stop gracefully.

```

请使用 service 或者 /etc/init.d/solr 启动，不建议使用 root 用户如下启动，会造成日志文件无权限等问题。

```
# /srv/solr/bin/solr start
Waiting up to 30 seconds to see Solr running on port 8983 [/]
Started Solr server on port 8983 (pid=56697). Happy searching!

```

我们将 solr 启动后交给 solr 用户完成。

```
# ps aux | grep solr
solr     62345 14.5  4.1 4103804 158960 ?      Sl   04:15   0:02 java -server -Xss256k -Xms512m -Xmx512m -XX:NewRatio=3 -XX:SurvivorRatio=4 -XX:TargetSurvivorRatio=90 -XX:MaxTenuringThreshold=8 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ConcGCThreads=4 -XX:ParallelGCThreads=4 -XX:+CMSScavengeBeforeRemark -XX:PretenureSizeThreshold=64m -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=50 -XX:CMSMaxAbortablePrecleanTime=6000 -XX:+CMSParallelRemarkEnabled -XX:+ParallelRefProcEnabled -verbose:gc -XX:+PrintHeapAtGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/srv/solr/server/logs/solr_gc.log -Djetty.port=8983 -DSTOP.PORT=7983 -DSTOP.KEY=solrrocks -Duser.timezone=UTC -Djetty.home=/srv/solr/server -Dsolr.solr.home=/srv/solr/server/solr -Dsolr.install.dir=/srv/solr -jar start.jar -XX:OnOutOfMemoryError=/srv/solr/bin/oom_solr.sh 8983 /srv/solr/server/logs --module=http
root     62458  0.0  0.0 112640   964 pts/0    S+   04:15   0:00 grep --color=auto solr

```

Solr Admin UI: [`192.168.4.1:8983/solr/#/`](http://192.168.4.1:8983/solr/#/)

## Core Admin

创建 Core

```

$ bin/solr create -c solr_docs

Setup new core instance directory:
/srv/solr/server/solr/solr_docs

Creating new core 'solr_docs' using command:
http://localhost:8983/solr/admin/cores?action=CREATE&name=solr_docs&instanceDir=solr_docs

{
  "responseHeader":{
    "status":0,
    "QTime":865},
  "core":"solr_docs"}

```

```

$ bin/solr create -c test

Setup new core instance directory:
/srv/solr/server/solr/test

Creating new core 'test' using command:
http://localhost:8983/solr/admin/cores?action=CREATE&name=test&instanceDir=test

{
  "responseHeader":{
    "status":0,
    "QTime":246},
  "core":"test"}

```

### Schema

```
cd server/solr/
cp -r configsets/sample_techproducts_configs new_core			

```

例 9.1. Solr - schema.xml

## 第 9 章 Solr

### *solr-5.3.0*

## 安装

安装

```
yum install -y unzip java-1.8.0-openjdk

wget http://www.us.apache.org/dist/lucene/solr/5.3.0/solr-5.3.0.tgz
tar zxvf solr-5.3.0.tgz
mv solr-5.3.0 /srv/
ln -s /srv/solr-5.3.0/ /srv/solr

adduser -d /srv/solr -c "Apache Solr" solr
chown solr:solr -R /srv/solr-5.3.0

cp /srv/solr-5.3.0/bin/init.d/solr /etc/init.d/
sed -i 's:/opt/solr:/srv/solr:' /etc/init.d/solr
sed -i 's:/var/solr:/srv/solr/bin:' /etc/init.d/solr

chkconfig --add  solr
chkconfig solr on

```

启动与停止

```
# service solr start
Waiting up to 30 seconds to see Solr running on port 8983 [/]
Started Solr server on port 8983 (pid=61909). Happy searching!

# service solr stop
Sending stop command to Solr running on port 8983 ... waiting 5 seconds to allow Jetty process 61909 to stop gracefully.

```

请使用 service 或者 /etc/init.d/solr 启动，不建议使用 root 用户如下启动，会造成日志文件无权限等问题。

```
# /srv/solr/bin/solr start
Waiting up to 30 seconds to see Solr running on port 8983 [/]
Started Solr server on port 8983 (pid=56697). Happy searching!

```

我们将 solr 启动后交给 solr 用户完成。

```
# ps aux | grep solr
solr     62345 14.5  4.1 4103804 158960 ?      Sl   04:15   0:02 java -server -Xss256k -Xms512m -Xmx512m -XX:NewRatio=3 -XX:SurvivorRatio=4 -XX:TargetSurvivorRatio=90 -XX:MaxTenuringThreshold=8 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ConcGCThreads=4 -XX:ParallelGCThreads=4 -XX:+CMSScavengeBeforeRemark -XX:PretenureSizeThreshold=64m -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=50 -XX:CMSMaxAbortablePrecleanTime=6000 -XX:+CMSParallelRemarkEnabled -XX:+ParallelRefProcEnabled -verbose:gc -XX:+PrintHeapAtGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/srv/solr/server/logs/solr_gc.log -Djetty.port=8983 -DSTOP.PORT=7983 -DSTOP.KEY=solrrocks -Duser.timezone=UTC -Djetty.home=/srv/solr/server -Dsolr.solr.home=/srv/solr/server/solr -Dsolr.install.dir=/srv/solr -jar start.jar -XX:OnOutOfMemoryError=/srv/solr/bin/oom_solr.sh 8983 /srv/solr/server/logs --module=http
root     62458  0.0  0.0 112640   964 pts/0    S+   04:15   0:00 grep --color=auto solr

```

Solr Admin UI: [`192.168.4.1:8983/solr/#/`](http://192.168.4.1:8983/solr/#/)

## 接口

### 查询

```
http://192.168.4.1:8983/solr/solr_docs/browse?q=results			

```

## FAQ

### NOTE: Please install lsof as this script needs it to determine if Solr is listening on port 8983.

```
# /srv/solr/bin/solr start

NOTE: Please install lsof as this script needs it to determine if Solr is listening on port 8983.

Started Solr server on port 8983 (pid=55873). Happy searching!

```

解决方法

```
yum install -y lsof

```

## Solr 1.3.0

http://lucene.apache.org/solr/

java 采用 apt-get 安装

例 9.2. /etc/profile.d/java.sh

```
################################################
### Java environment by neo
################################################
export JAVA_HOME=/usr
export JRE_HOME=/usr
export PATH=$PATH:/usr/local/apache-tomcat/bin/:/usr/local/jetty-6.1.18/bin
export CLASSPATH="./:/usr/share/java/:/usr/local/apache-solr/example/multicore/lib"
export JAVA_OPTS="-Xms128m -Xmx1024m"

```

### Embedded Jetty

```

wget http://apache.freelamp.com/lucene/solr/1.3.0/apache-solr-1.3.0.tgz
tar zxvf apache-solr-1.3.0.tgz
ln -s apache-solr-1.3.0 ../apache-solr
cd ../apache-solr/example/
java -jar start.jar

```

multicore: java -Dsolr.solr.home=multicore -jar start.jar

### Jetty

http://jetty.mortbay.org/jetty/

过程 9.1. apt-get install

1.  install

    ```

    $ sudo apt-get install libxpp3-java
    $ sudo apt-get install solr-jetty

    ```

2.  firewall

    ```

    $ sudo ufw allow 8280

    ```

3.  Testing.

    http://172.16.0.1:8280/

    http://172.16.0.1:8280/admin/ (user:admin, passwd:admin)

过程 9.2. source codes install

*   download

    ```

    wget http://dist.codehaus.org/jetty/jetty-6.1.18/jetty-6.1.18.zip

    ```

### Tomcat

http://tomcat.apache.org/

1.  download

    ```

    cd /usr/local/src

    wget http://apache.etoak.com/tomcat/tomcat-6/v6.0.20/bin/apache-tomcat-6.0.20.tar.gz
    wget http://apache.freelamp.com/lucene/solr/1.3.0/apache-solr-1.3.0.tgz

    tar zxvf apache-tomcat-6.0.20.tar.gz
    ln -s apache-tomcat-6.0.20 ../apache-tomcat

    tar zxvf apache-solr-1.3.0.tgz
    ln -s apache-solr-1.3.0 ../apache-solr

    ```

2.  solr.xml

    ```

    vim /usr/local/apache-tomcat/conf/Catalina/localhost/solr.xml

    <Context docBase="/usr/local/apache-solr/dist/apache-solr-1.3.0.war" debug="0" crossContext="true" >
       <Environment name="solr/home" type="java.lang.String" value="/usr/local/apache-solr/example/solr" override="true" />
    </Context>

    ```

### solr-php-client

http://code.google.com/p/solr-php-client/

```
wget http://solr-php-client.googlecode.com/files/SolrPhpClient.2009-03-11.tgz
tar zxvf SolrPhpClient.2009-03-11.tgz
sudo mv SolrPhpClient/Apache /usr/share/php/

```

### multicore

solr.xml

```

vim /usr/local/apache-solr/example/multicore/solr.xml

<?xml version="1.0" encoding="UTF-8" ?>
<solr persistent="false">
  <cores adminPath="/admin/cores">
    <core name="core0" instanceDir="core0" />
    <core name="core1" instanceDir="core1" />

    <core name="article" instanceDir="article" />

  </cores>
</solr>

```

core directory and config file

```

mkdir -p article/conf

vim article/conf/solrconfig.xml

<?xml version="1.0" encoding="UTF-8" ?>
<config>
  <updateHandler class="solr.DirectUpdateHandler2" />
  <requestDispatcher handleSelect="true" >
    <requestParsers enableRemoteStreaming="false" multipartUploadLimitInKB="2048" />
  </requestDispatcher>
  <requestHandler name="standard" class="solr.StandardRequestHandler" default="true" />
  <requestHandler name="/update" class="solr.XmlUpdateRequestHandler" />
  <requestHandler name="/admin/" class="org.apache.solr.handler.admin.AdminHandlers" />
  <admin>
    <defaultQuery>solr</defaultQuery>
  </admin>
</config>

vim article/conf/schema.xml

<?xml version="1.0" ?>
<schema name="example core zero" version="1.1">
  <types>
   <fieldType name="sint" class="solr.SortableIntField" sortMissingLast="true" omitNorms="true"/>
   <fieldtype name="string"  class="solr.StrField" sortMissingLast="true" omitNorms="true"/>
   <fieldType name="date" class="solr.DateField" sortMissingLast="true" omitNorms="true"/>
   <fieldType name="text" class="solr.TextField" positionIncrementGap="100" />
  </types>
 <fields>
  <!-- general -->
  <field name="id"      type="sint"   indexed="true"  stored="true"  multiValued="false" required="true"/>
  <field name="type"    type="string"   indexed="true"  stored="true"  multiValued="false" />
  <field name="name"    type="string"   indexed="true"  stored="true"  multiValued="false" />
  <field name="title"   type="string"   indexed="true"  stored="true"  multiValued="false" />
  <field name="content"   type="text"   indexed="true"  stored="true"  multiValued="false" />
  <field name="timestamp" type="date" indexed="true" stored="true" default="NOW"/>
 </fields>
 <!-- field to use to determine and enforce document uniqueness. -->
 <uniqueKey>id</uniqueKey>
 <!-- field for the QueryParser to use when an explicit fieldname is absent -->
 <defaultSearchField>content</defaultSearchField>
 <!-- SolrQueryParser configuration: defaultOperator="AND|OR" -->
 <solrQueryParser defaultOperator="OR"/>
	<copyField source="title" dest="content"/>
	<copyField source="name" dest="content"/>
</schema>

```

commit datas

```

vim test.xml

<add>
	<doc>
	  <field name="id">1</field>
	  <field name="name">Hello world</field>
	</doc>

	<doc>
	  <field name="id">2</field>
	  <field name="title">Title Hello world</field>
	</doc>

	<doc>
	  <field name="id">3</field>
	  <field name="name">Hello world 1</field>
	  <field name="content">Content 1</field>
	</doc>

	<doc>
	  <field name="id">4</field>
	  <field name="name">Name Neo</field>
	</doc>

	<doc>
	  <field name="id">5</field>
	  <field name="name">Last Chan</field>
	</doc>
</add>

java -Durl=http://localhost:8983/solr/article/update -Dcommit=yes -jar ../exampledocs/post.jar test.xml

```

### 中文分词

#### ChineseTokenizerFactory

```

<fieldType name="text" class="solr.TextField" >
    <analyzer>
        <tokenizer class="org.apache.solr.analysis.ChineseTokenizerFactory"/>
    </analyzer>
</fieldType>

```

#### CJK

```

<fieldType name="text" class="solr.TextField" positionIncrementGap="100">
	<analyzer>
    	<tokenizer class="solr.CJKTokenizerFactory"/>
	</analyzer>
</fieldType>

```

#### mmseg4j

http://code.google.com/p/mmseg4j/

install

```

$ cd /usr/local/src/
$ wget http://mmseg4j.googlecode.com/files/mmseg4j-1.7.2.zip
$ unzip mmseg4j-1.7.2.zip
$ mkdir /usr/local/apache-solr/example/multicore/lib
$ cp /usr/local/src/mmseg4j-1.7.2/mmseg4j-all-1.7.2.jar /usr/local/apache-solr/example/multicore/lib
$ cd mmseg4j-1.7.2/

```

test

```
$ java -Dmmseg.dic.path=/usr/local/apache-solr/example/solr -jar mmseg4j-all-1.7.2.jar 这里是字符串
$ java -Dmmseg.dic.path=/usr/local/apache-solr/example/solr -cp .:mmseg4j-all-1.7.2.jar com.chenlb.mmseg4j.example.Simple 这里是字符串
$ java -Dmmseg.dic.path=/usr/local/apache-solr/example/solr -cp .:mmseg4j-all-1.7.2.jar com.chenlb.mmseg4j.example.MaxWord 这里是字符串

```

mmseg4j 在 solr 中主要支持两个参数：mode、dicPath。mode 表示是什么模式分词（有效值：simplex、complex、max-word，如果输入了无效的默认用 max-word。）。dicPath 是词库目录可以是绝对目录，也可以是相对目录（是相对 solr.home 目录下的，dic 就会在 solr.home/dic 目录下找词库文件），如果不指定就是默认在 CWD/data 目录（程序运行当前目录的 data 子目录）下找。

分词例子

```

<fieldtype name="textComplex" class="solr.TextField">
	<analyzer>
		<tokenizer class="com.chenlb.mmseg4j.solr.MMSegTokenizerFactory"
			mode="complex" dicPath="dic">
		</tokenizer>
	</analyzer>
</fieldtype>

<fieldtype name="textMaxWord" class="solr.TextField">
	<analyzer>
		<tokenizer class="com.chenlb.mmseg4j.solr.MMSegTokenizerFactory"
			mode="max-word" dicPath="dic">
		</tokenizer>
	</analyzer>
</fieldtype>

<fieldtype name="textSimple" class="solr.TextField">
	<analyzer>
		<tokenizer class="com.chenlb.mmseg4j.solr.MMSegTokenizerFactory"
			mode="simple" dicPath="/usr/local/apache-solr/example/solr/my_dic">
		</tokenizer>
	</analyzer>
</fieldtype>

```

添加到 schema.xml

```

<fieldType name="text" class="solr.TextField" positionIncrementGap="100" >
    <analyzer>
        <tokenizer class="com.chenlb.mmseg4j.solr.MMSegTokenizerFactory" mode="complex" dicPath="dic"/>
        <filter class="solr.LowerCaseFilterFactory"/>
    </analyzer>
</fieldType>

```

http://localhost:8080/solr/admin/analysis.jsp 在 Field 的下拉菜单选择 name，然后在应用输入 complex。可以看 mmseg4j 的分词的结果.

#### 中文分词“庖丁解牛” Paoding Analysis

```
$ cd /usr/local/src/
$ mkdir paoding-analysis-2.0.4-beta
$ cd paoding-analysis-2.0.4-beta/
$ wget http://paoding.googlecode.com/files/paoding-analysis-2.0.4-beta.zip
$ unzip paoding-analysis-2.0.4-beta.zip
$ cp paoding-analysis.jar /usr/local/apache-solr/example/multicore/lib/

```

ChineseTokenizerFactory

## 第 10 章 Nutch

http://lucene.apache.org/nutch/

How to Setup Nutch and Hadoop

http://wiki.apache.org/nutch/NutchHadoopTutorial

1.  下载

    ```
    $ cd /usr/local/src/
    $ wget http://apache.etoak.com/lucene/nutch/nutch-1.0.tar.gz
    $ tar zxvf nutch-1.0.tar.gz
    $ sudo cp -r nutch-1.0 ..
    $ cd ..
    $ sudo ln -s nutch-1.0 apache-nutch

    ```

2.  创建文件 myurl

    ```
    $ cd apache-nutch
    $ mkdir urls
    $ vim urls/myurl
    http://netkiller.8800.org/

    ```

3.  配置文件 crawl-urlfilter.txt

    编辑 conf/crawl-urlfilter.txt 文件，修改 MY.DOMAIN.NAME 部分，把它替换为你想要抓取的域名

    ```
    $ cp conf/crawl-urlfilter.txt conf/crawl-urlfilter.txt.old
    $ vim conf/crawl-urlfilter.txt

    # accept hosts in MY.DOMAIN.NAME
    +^http://([a-z0-9]*\.)*MY.DOMAIN.NAME/
    修改为：
    # accept hosts in MY.DOMAIN.NAME
    +^http://([a-z0-9]*\.)*netkiller.8800.org/

    ```

4.  http.agent.name

    ```

    $ vim conf/nutch-site.xml
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <!-- Put site-specific property overrides in this file. -->

    <configuration>

    <property>
      <name>http.agent.name</name>
      <value>Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5</value>
      <description>HTTP 'User-Agent' request header. MUST NOT be empty -
      please set this to a single word uniquely related to your organization.

      NOTE: You should also check other related properties:

        http.robots.agents
        http.agent.description
        http.agent.url
        http.agent.email
        http.agent.version

      and set their values appropriately.

      </description>
    </property>

    <property>
      <name>http.agent.description</name>
      <value></value>
      <description>Further description of our bot- this text is used in
      the User-Agent header.  It appears in parenthesis after the agent name.
      </description>
    </property>

    <property>
      <name>http.agent.url</name>
      <value>http://netkiller.8800.org/robot.html</value>
      <description>A URL to advertise in the User-Agent header.  This will
       appear in parenthesis after the agent name. Custom dictates that this
       should be a URL of a page explaining the purpose and behavior of this
       crawler.
      </description>
    </property>

    <property>
      <name>http.agent.email</name>
      <value>openunix@163.com</value>
      <description>An email address to advertise in the HTTP 'From' request
       header and User-Agent header. A good practice is to mangle this
       address (e.g. 'info at example dot com') to avoid spamming.
      </description>
    </property>

    </configuration>

    ```

5.  运行以下命令行开始工作

    **$ bin/nutch crawl urls -dir crawl -depth 3 -threads 5**

    ```

    bin/nutch crawl <your_url> -dir <your_dir> -depth 2 -threads 4 >&logs/logs1.log

    urls 存放需要爬行的 url 文件的目录，即目录/nutch/urls。
    -dir  dirnames    	设置保存所抓取网页的目录.
    -depth  depth 		表明抓取网页的层次深度
    -delay  delay		表明访问不同主机的延时，单位为“秒”
    -threads  threads  	表明需要启动的线程数
    -topN 50	topN	一个网站保存的最大页面数。

    $ nohup bin/nutch crawl /usr/local/apache-nutch/urls -dir /usr/local/apache-nutch/crawl -depth 5 -threads 50 -topN 50 > /tmp/nutch.log &

    ```

6.  depoly

    ```

    $ cd /usr/local/apache-tomcat/conf/Catalina/localhost
    $ vim nutch.xml
    <Context docBase="/usr/local/apache-nutch/nutch-1.0.war" debug="0" crossContext="true" >
    </Context>

    ```

    searcher.dir

    ```

    $ vim /usr/local/apache-tomcat/webapps/nutch/WEB-INF/classes/nutch-site.xml

    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <!-- Put site-specific property overrides in this file. -->

    <configuration>
        <property>
            <name>searcher.dir</name>
            <value>/usr/local/apache-nutch/crawl</value>
        </property>
    </configuration>

    ```

    test

    http://172.16.0.1:8080/nutch/

## 第 11 章 Sphinx

http://sphinxsearch.com/

```
sudo apt-get install sphinxsearch

```

/etc/sphinxsearch/sphinx.conf

```
sudo cp /etc/sphinxsearch/sphinx-min.conf.dist /etc/sphinxsearch/sphinx.conf

```

创建测试数据库并导入测试数据

```

$ wget http://sphinxsearch.googlecode.com/svn/trunk/example.sql
$ mysql -h localhost -uroot -p < example.sql
$ mysql -h localhost -uroot -p
CREATE USER 'test'@'localhost' IDENTIFIED BY  '';
GRANT SELECT ON test.* TO 'test'@'localhost';
FLUSH PRIVILEGES;
mysql> quit

$ echo "select * from documents" | mysql -utest -p test
Enter password:
id      group_id        group_id2       date_added      title   content
1       1       5       2011-02-12 15:29:34     test one        this is my test document number one. also checking search within phrases.
2       1       6       2011-02-12 15:29:34     test two        this is my test document number two
3       2       7       2011-02-12 15:29:34     another doc     this is another group
4       2       8       2011-02-12 15:29:34     doc number four this is to test groups

```

创建索引

**sudo indexer <index>**

```

$ sudo indexer test1

Sphinx 0.9.8.1-release (r1533)
Copyright (c) 2001-2008, Andrew Aksyonoff

using config file '/etc/sphinxsearch/sphinx.conf'...
indexing index 'test1'...
collected 4 docs, 0.0 MB
sorted 0.0 Mhits, 100.0% done
total 4 docs, 193 bytes
total 0.012 sec, 16531.05 bytes/sec, 342.61 docs/sec

```

```
$ sudo /etc/init.d/sphinxsearch start
Starting sphinx: Sphinx 0.9.8.1-release (r1533)
Copyright (c) 2001-2008, Andrew Aksyonoff

using config file '/etc/sphinxsearch/sphinx.conf'...
creating server socket on 0.0.0.0:3312
sphinx.

```

测试

search "keyword"

```

$ search test
Sphinx 0.9.8.1-release (r1533)
Copyright (c) 2001-2008, Andrew Aksyonoff

using config file '/etc/sphinxsearch/sphinx.conf'...
index 'test1': query 'test ': returned 3 matches of 3 total in 0.000 sec

displaying matches:
1\. document=1, weight=2, group_id=1, date_added=Sat Feb 12 15:29:34 2011
        id=1
        group_id=1
        group_id2=5
        date_added=2011-02-12 15:29:34
        title=test one
        content=this is my test document number one. also checking search within phrases.
2\. document=2, weight=2, group_id=1, date_added=Sat Feb 12 15:29:34 2011
        id=2
        group_id=1
        group_id2=6
        date_added=2011-02-12 15:29:34
        title=test two
        content=this is my test document number two
3\. document=4, weight=1, group_id=2, date_added=Sat Feb 12 15:29:34 2011
        id=4
        group_id=2
        group_id2=8
        date_added=2011-02-12 15:29:34
        title=doc number four
        content=this is to test groups

words:
1\. 'test': 3 documents, 5 hits

```

```
wget http://sphinxsearch.googlecode.com/svn/trunk/api/sphinxapi.php
wget http://sphinxsearch.googlecode.com/svn/trunk/api/test.php
php test.php test

```

## 第 12 章 Lucene

http://lucene.apache.org/

## 第 13 章 MG4J

http://mg4j.dsi.unimi.it/

## 第 14 章 PhpDig

http://www.phpdig.net/

PhpDig is a web spider and search engine written in PHP, using a MySQL database and flat file support. PhpDig builds a glossary with words found in indexed pages. On a search query, it displays a result page containing the search keys, ranked by occurrence.

## 第 15 章 Mahout

http://mahout.apache.org/